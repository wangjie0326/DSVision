import re
from enum import Enum
from typing import List,Optional, Tuple

class TokenType(Enum):
    """Token type"""
    #å…³é”®å­—
    SEQUENTIAL = "SEQUENTIAL"
    LINKED = "LINKED"
    STACK = "STACK"
    QUEUE = "QUEUE"
    BINARY = "BINARY"
    BST = "BST"
    AVL = "AVL"
    HUFFMAN = "HUFFMAN"

    #æ“ä½œå…³é”®å­—
    INIT = "INIT"
    INSERT = "INSERT"
    DELETE = "DELETE"
    SEARCH = "SEARCH"
    CLEAR = "CLEAR"
    SAVE = "SAVE"
    LOAD = "LOAD"
    EXPORT = "EXPORT"
    IMPORT = "IMPORT"

    #æ ˆ/é˜Ÿåˆ—æ“ä½œ
    PUSH = "PUSH"
    POP = "POP"
    PEEK = "PEEK"
    ENQUEUE = "ENQUEUE"
    DEQUEUE = "DEQUEUE"
    FRONT = "FRONT"
    REAR = "REAR"

    #æ ‘æ“ä½œ
    BUILD = "BUILD"
    TRAVERSE = "TRAVERSE"
    HEIGHT = "HEIGHT"
    MIN = "MIN"
    MAX = "MAX"
    REVERSE = "REVERSE"

    #HUFFMAN
    BUILD_TEXT = "BUILD_TEXT"
    BUILD_FREQ = "BUILD_FREQ"
    BUILD_NUMBERS = "BUILD_NUMBERS"  # ğŸ”¥ æ–°å¢ï¼šçº¯æ•°å­—æ¨¡å¼
    ENCODE = "ENCODE"
    DECODE = "DECODE"
    SHOW_CODES = "SHOW_CODES"

    #é“¾è¡¨æ“ä½œ
    INSERT_HEAD = "INSERT_HEAD"
    INSERT_TAIL = "INSERT_TAIL"
    DELETE_HEAD = "DELETE_HEAD"
    DELETE_TAIL = "DELETE_TAIL"

    #é«˜çº§ç‰¹æ€§
    FOR = "FOR"
    IN = "IN"
    IF = "IF"
    TRY = "TRY"
    CATCH = "CATCH"
    RANGE = "RANGE"
    SPEED = "SPEED"
    PAUSE = "PAUSE"

    #éå†æ–¹å¼
    PREORDER = "PREORDER"
    INORDER = "INORDER"
    POSTORDER = "POSTORDER"
    LEVELORDER = "LEVELORDER"

    #ä½ç½®å…³é”®å­—
    AT = "AT"
    GET = "GET"
    SIZE = "SIZE"

    #ç¬¦å·
    LBRACE = "LBRACE"
    RBRACE = "RBRACE"
    LBRACKET = "LBRACKET"
    RBRACKET = "RBRACKET"
    LPAREN = "LPAREN"
    RPAREN = "RPAREN"
    COMMA = "COMMA"
    COLON = "COLON"
    DOT = "DOT"

    #å­—é¢é‡
    NUMBER = "NUMBER"
    STRING = "STRING"
    IDENTIFIER = "IDENTIFIER"
    # ç‰¹æ®Š
    NEWLINE = "NEWLINE"
    EOF = "EOF"
    COMMENT = "COMMENT"

class Token:
    """Token type"""
    def __init__(self, type: TokenType, value: any, line: int, column: int):
        self.type = type
        self.value = value
        self.line = line
        self.column = column

    def __repr__(self):
        return f"Token({self.type.name}, {self.value!r}, {self.line}:{self.column})"

class Lexer:
    """è¯æ³•åˆ†æå™¨"""
    #å…³é”®è¯æ˜ å°„
    KEYWORDS = {
        'sequential': TokenType.SEQUENTIAL,
        'linked': TokenType.LINKED,
        'stack': TokenType.STACK,
        'queue': TokenType.QUEUE,
        'binary': TokenType.BINARY,
        'bst': TokenType.BST,
        'avl': TokenType.AVL,
        'huffman': TokenType.HUFFMAN,

        'init': TokenType.INIT,
        'insert': TokenType.INSERT,
        'delete': TokenType.DELETE,
        'search': TokenType.SEARCH,
        'clear': TokenType.CLEAR,
        'export': TokenType.EXPORT,
        'import': TokenType.IMPORT,
        'save': TokenType.SAVE,
        'load': TokenType.LOAD,

        'push':TokenType.PUSH,
        'pop': TokenType.POP,
        'peek': TokenType.PEEK,
        'enqueue': TokenType.ENQUEUE,
        'dequeue': TokenType.DEQUEUE,
        'front': TokenType.FRONT,
        'rear': TokenType.REAR,

        'build': TokenType.BUILD,
        'traverse': TokenType.TRAVERSE,
        'height': TokenType.HEIGHT,
        'min': TokenType.MIN,
        'max': TokenType.MAX,
        'reverse': TokenType.REVERSE,

        'build_text': TokenType.BUILD_TEXT,
        'build_freq': TokenType.BUILD_FREQ,
        'build_numbers': TokenType.BUILD_NUMBERS,  # ğŸ”¥ æ–°å¢
        'encode': TokenType.ENCODE,
        'decode': TokenType.DECODE,
        'show_codes': TokenType.SHOW_CODES,

        'insert_head': TokenType.INSERT_HEAD,
        'insert_tail': TokenType.INSERT_TAIL,
        'delete_head': TokenType.DELETE_HEAD,
        'delete_tail': TokenType.DELETE_TAIL,

        'for': TokenType.FOR,
        'in': TokenType.IN,
        'if': TokenType.IF,
        'try': TokenType.TRY,
        'catch': TokenType.CATCH,
        'range': TokenType.RANGE,
        'speed': TokenType.SPEED,
        'pause': TokenType.PAUSE,

        'preorder': TokenType.PREORDER,
        'inorder': TokenType.INORDER,
        'postorder': TokenType.POSTORDER,
        'levelorder': TokenType.LEVELORDER,

        'at': TokenType.AT,
        'get': TokenType.GET,
        'size': TokenType.SIZE,
    }

    def __init__(self,code: str):
        self.code = code
        self.pos = 0
        self.line = 1
        self.column = 1
        self.tokens: List[Token] = []

    def error(self,message: str):
        """æŠ¥é”™"""
        raise SyntaxError(f"[Lexer Error] Line {self.line}:{self.column} - {message}")

    def advance(self):
        """è¯»å–å¹¶ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå­—ç¬¦"""
        if self.pos >= len(self.code):
            return None
        char = self.code[self.pos]
        self.pos += 1

        if char == '\n':
            self.line += 1
            self.column += 1
        else:
            self.column += 1

        return char

    def peek(self, offset: int = 0) -> Optional[str]:
        """æŸ¥çœ‹å­—ç¬¦ä½†ä¸ç§»åŠ¨ä½ç½®"""
        pos = self.pos + offset
        if pos < len(self.code):
            return self.code[pos]
        return None

    def skip_whitespace(self):
        """è·³è¿‡ç©ºæ ¼ï¼ˆä¸åŒ…æ‹¬æ¢è¡Œï¼‰"""
        while self.peek() and self.peek() in ' \t\r':
            self.advance()

    def skip_comment(self):
        """è·³è¿‡æ³¨é‡Š"""
        #å•è¡Œæ³¨é‡Š
        if self.peek() == '/' and self.peek(1) == '/':
            while self.peek(2) and self.peek() == '/':
                self.advance()
            return True

        #å¤šè¡Œæ³¨é‡Š
        if self.peek() == '/' and self.peek(1) == '/':
            self.advance() # /
            self.advance() # *

            while True:
                if self.peek() is None:
                    self.error("æœªé—­åˆçš„å¤šè¡Œæ³¨é‡Š")
                if self.peek() == '*' and self.peek(1) == '/':
                    self.advance()  # *
                    self.advance()  # /
                    break
                self.advance()
            return True
        return False

    def read_number(self) -> Token:
        """è¯»å–æ•°å­—"""
        start_line = self.line
        start_column = self.column
        num_str = ''

        # è´Ÿå·
        if self.peek() == '-':
            num_str += self.advance()

        # æ•´æ•°éƒ¨åˆ†
        while self.peek() and self.peek().isdigit():
            num_str += self.advance()

        # å°æ•°éƒ¨åˆ†
        if self.peek() == '.':
            num_str += self.advance()
            while self.peek() and self.peek().isdigit():
                num_str += self.advance()
            return Token(TokenType.NUMBER, float(num_str), start_line, start_column)

        return Token(TokenType.NUMBER, int(num_str), start_line, start_column)

    def read_string(self) -> Token:
        """è¯»å–å­—ç¬¦ä¸²"""
        start_line = self.line
        start_column = self.column
        quote = self.advance()  # " æˆ– '
        string = ''

        while True:
            char = self.peek()
            if char is None:
                self.error(f"æœªé—­åˆçš„å­—ç¬¦ä¸²")
            if char == quote:
                self.advance()
                break
            if char == '\\':
                self.advance()
                next_char = self.advance()
                if next_char == 'n':
                    string += '\n'
                elif next_char == 't':
                    string += '\t'
                elif next_char == '\\':
                    string += '\\'
                elif next_char == quote:
                    string += quote
                else:
                    string += next_char
            else:
                string += self.advance()

        return Token(TokenType.STRING, string, start_line, start_column)

    def read_identifier(self) -> Token:
        """è¯»å–æ ‡è¯†ç¬¦æˆ–å…³é”®å­—"""
        start_line = self.line
        start_column = self.column
        identifier = ''

        while self.peek and (self.peek().isalnum() or self.peek() == '_'):
            identifier += self.advance()

        # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®å­—
        token_type = self.KEYWORDS.get(identifier.lower(), TokenType.IDENTIFIER)

        return Token(token_type, identifier, start_line, start_column)

    def tokenize(self) -> List[Token]:
        """è¯æ³•åˆ†æï¼Œè¿”å›tokenåˆ—è¡¨"""

        while self.pos < len(self.code):
            self.skip_whitespace()

            #è·³è¿‡æ³¨é‡Š
            if self.skip_comment():
                continue

            char = self.peek()

            if char is None:
                break

            #æ¢è¡Œ
            if char == '\n':
                line = self.line
                col = self.column
                self.advance()
                self.tokens.append(Token(TokenType.NEWLINE, '\n', line, col))
                continue

            # æ•°å­—
            if char.isdigit() or (char == '-' and self.peek(1) and self.peek(1).isdigit()):
                self.tokens.append(self.read_number())
                continue

            # å­—ç¬¦ä¸²
            if char in '"\'':
                self.tokens.append(self.read_string())
                continue

            # æ ‡è¯†ç¬¦æˆ–å…³é”®å­—
            if char.isalpha() or char == '_':
                self.tokens.append(self.read_identifier())
                continue

            # ç¬¦å·
            line = self.line
            col = self.column

            if char == '{':
                self.advance()
                self.tokens.append(Token(TokenType.LBRACE, '{', line, col))
            elif char == '}':
                self.advance()
                self.tokens.append(Token(TokenType.RBRACE, '}', line, col))
            elif char == '[':
                self.advance()
                self.tokens.append(Token(TokenType.LBRACKET, '[', line, col))
            elif char == ']':
                self.advance()
                self.tokens.append(Token(TokenType.RBRACKET, ']', line, col))
            elif char == '(':
                self.advance()
                self.tokens.append(Token(TokenType.LPAREN, '(', line, col))
            elif char == ')':
                self.advance()
                self.tokens.append(Token(TokenType.RPAREN, ')', line, col))
            elif char == ',':
                self.advance()
                self.tokens.append(Token(TokenType.COMMA, ',', line, col))
            elif char == ':':
                self.advance()
                self.tokens.append(Token(TokenType.COLON, ':', line, col))
            elif char == '.':
                self.advance()
                self.tokens.append(Token(TokenType.DOT, '.', line, col))
            else:
                self.error(f"æœªçŸ¥å­—ç¬¦: '{char}'")

        # æ·»åŠ EOFæ ‡è®°
        self.tokens.append(Token(TokenType.EOF, None, self.line, self.column))

        return self.tokens


if __name__ == "__main__":
    test_code = """
    Sequential myList {
        init [1, 2, 3, 4, 5]
        insert 10 at 2
        search 10
        save "mylist.json"
    }
    """

    lexer = Lexer(test_code)
    tokens = lexer.tokenize()

    print("=== Tokenæµ ===")
    for token in tokens:
        if token.type != TokenType.NEWLINE:
            print(token)




